function [pred] = NeuralNetwork(x1)
%MYNEURALNETWORKFUNCTION neural network simulation function.
%
% Auto-generated by MATLAB, 24-Mar-2021 14:11:48.
%
% [y1] = myNeuralNetworkFunction(x1) takes these arguments:
%   x = Qx2 matrix, input #1
% and returns:
%   y = Qx10 matrix, output #1
% where Q is the number of samples.

%#ok<*RPMT0>

% ===== NEURAL NETWORK CONSTANTS =====

% Input 1
x1_step1.xoffset = [1;0];
x1_step1.gain = [0.4;0.0380228136882129];
x1_step1.ymin = -1;

% Layer 1
b1 = [4.588587186727106193;-1.7360811642338671579;-1.6334451679183457085;-0.68569934037375535052;-2.6610515562238035159;-2.4705974384802060762];
IW1_1 = [-0.31288197276988499729 5.6526137392294657502;0.71854174179855989646 4.4012730432080529042;2.7566829515608510803 -5.1173591520868688676;0.72301375810666979049 -5.5819859828289750681;-4.6159877370259945195 2.3218246766775911816;-0.41265912995874876046 -5.3128760769925102991];

% Layer 2
b2 = [0.26657832342350951116;-1.5604292281183240743;0.45811713341140858358;0.59871027250882924431;-0.14713529382514070609;0.43780113050329683144;-0.9561378166052494354;0.68678982806675226147;0.63155895972482734013;0.21836227869436874993];
LW2_1 = [-5.2487933107309840963 -0.94365263857909120748 -0.5538387382705842743 1.5977265889065637161 -3.6741215808711245039 6.7461738307010099547;-0.025327530124627697927 -0.37352577115577512146 4.7584224113742594753 3.9139527814582892873 -1.600815876901720447 3.065572030643149315;1.5214493050824700049 -0.27557439687349261082 3.2192477384682050001 3.9501060759953157131 1.0800986402040775314 1.3509044317670091395;2.5884408457500871492 -0.61055941090550303052 1.7877887492404103131 2.9421885351266889863 0.48242227081630600916 -1.1517801464623396157;-0.059432713780178239582 0.30525994212797386007 -2.4532593051383817517 3.4460056860407433099 -2.2478637179606963947 -2.0173043401523047535;0.30299842988186853621 -1.1773237970715599854 -1.7724621984793851315 0.72243373462707660693 0.12209286012223363738 -3.3911087552761807196;1.1231116763838093764 -2.2974483081197449863 -0.75099442200473809361 -3.8217462845299849938 0.86972426536961044974 -2.3333665677607853262;1.3379678210145415918 -0.94675320275951113658 -1.6265423203940996277 -2.2302901838116047095 1.4084899970608020237 -2.2182285034009332492;-0.70904030660673855735 -0.11440132497044080973 -2.0686257094484106211 -4.7570290118676696878 -0.53286302947592301127 -0.9141009880791601816;0.29408804228681945103 2.9503677495965048294 -2.2968401860021194771 -6.4455029415675726412 3.3931455512346788694 -2.2226329451448845198];

% ===== SIMULATION ========

% Dimensions
Q = size(x1,1); % samples

% Input 1
x1 = x1';
xp1 = mapminmax_apply(x1,x1_step1);

% Layer 1
a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*xp1);

% Layer 2
a2 = softmax_apply(repmat(b2,1,Q) + LW2_1*a1);

% Output 1
y1 = a2;
y1 = y1';
[~,pred]=max(y1);
pred=pred-1;
end

% ===== MODULE FUNCTIONS ========

% Map Minimum and Maximum Input Processing Function
function y = mapminmax_apply(x,settings)
y = bsxfun(@minus,x,settings.xoffset);
y = bsxfun(@times,y,settings.gain);
y = bsxfun(@plus,y,settings.ymin);
end

% Competitive Soft Transfer Function
function a = softmax_apply(n,~)
if isa(n,'gpuArray')
    a = iSoftmaxApplyGPU(n);
else
    a = iSoftmaxApplyCPU(n);
end
end
function a = iSoftmaxApplyCPU(n)
nmax = max(n,[],1);
n = bsxfun(@minus,n,nmax);
numerator = exp(n);
denominator = sum(numerator,1);
denominator(denominator == 0) = 1;
a = bsxfun(@rdivide,numerator,denominator);
end
function a = iSoftmaxApplyGPU(n)
nmax = max(n,[],1);
numerator = arrayfun(@iSoftmaxApplyGPUHelper1,n,nmax);
denominator = sum(numerator,1);
a = arrayfun(@iSoftmaxApplyGPUHelper2,numerator,denominator);
end
function numerator = iSoftmaxApplyGPUHelper1(n,nmax)
numerator = exp(n - nmax);
end
function a = iSoftmaxApplyGPUHelper2(numerator,denominator)
if (denominator == 0)
    a = numerator;
else
    a = numerator ./ denominator;
end
end

% Sigmoid Symmetric Transfer Function
function a = tansig_apply(n,~)
a = 2 ./ (1 + exp(-2*n)) - 1;
end
